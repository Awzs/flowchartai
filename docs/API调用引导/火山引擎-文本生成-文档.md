# 文本生成
语言大模型具备文本理解和文字对话的能力。当传入文本信息时，大模型可理解信息并回复。通过这篇教程，您可学习如何使用模型服务 API，调用模型理解文本信息，生成文本内容。

## 前提条件

- [获取 API Key](https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey)
	- 使用 Access Key 鉴权请参考 [Access Key 签名鉴权](https://www.volcengine.com/docs/82379/1298459#21bff83b) 。
- [开通模型服务](https://console.volcengine.com/ark/openManagement)
- 在 [模型列表](https://www.volcengine.com/docs/82379/1330310) 获取所需 Model ID
	- 通过 Endpoint ID 调用模型服务，请参考 [获取 Endpoint ID（创建自定义推理接入点）](https://www.volcengine.com/docs/82379/1099522) 。

## 快速开始

```python
import os
# Install SDK:  pip install 'volcengine-python-sdk[ark]'
from volcenginesdkarkruntime import Ark

client = Ark(
    # The base URL for model invocation
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    # Get API Key：https://console.volcengine.com/ark/region:ark+cn-beijing/apikey
    api_key=os.getenv('ARK_API_KEY'),
)

completion = client.chat.completions.create(
    # Replace with Model ID
    model = "doubao-seed-1-6-251015",
    messages = [
        {"role": "user", "content": "请将下面内容进行结构化处理：火山方舟是火山引擎推出的大模型服务平台，提供模型训练、推理、评测、精调等全方位功能与服务，并重点支撑大模型生态。 火山方舟通过稳定可靠的安全互信方案，保障模型提供方的模型安全与模型使用者的信息安全，加速大模型能力渗透到千行百业，助力模型提供方和使用者实现商业新增长。"},
    ],
)

print(completion.choices[0].message.content)
Python
```

## 模型回复预览：

```shell
**一、火山方舟的所属与定位**
火山方舟是火山引擎推出的大模型服务平台。

**二、火山方舟的功能与服务**
1. 提供全方位功能
   - 包括模型训练、推理、评测、精调等功能与服务。
2. 重点支撑
   - 重点支撑大模型生态。

**三、火山方舟的安全保障与作用**
1. 安全保障
   - 通过稳定可靠的安全互信方案，保障模型提供方的模型安全与模型使用者的信息安全。
2. 作用
   - 加速大模型能力渗透到千行百业，助力模型提供方和使用者实现商业新增长。
Shell
```

## 提示词工程

提示词（Prompt）是输入给模型的信息，模型会根据提示词来进行推理，生成回复内容。正确设计和编写提示词，如提供说明、示例、好的规范等方法可提高模型输出的质量和准确性。而进行提示词优化的工作也被称为提示词工程（Prompt Engineering）。

> 我们为您提供了如何编辑好的提示词的一些实践 [Prompt 最佳实践](https://www.volcengine.com/docs/82379/1221660) ，您可基于实践来优化提示词，以获得更好的回复。

在 [Chat API](https://www.volcengine.com/docs/82379/1494384) 中，通过 `messages` 对象将信息传入给模型，其中 `role` 字段定义信息传入的角色， `content` 承载消息内容。模型会结合传入的角色和信息来理解内容，并生成对应的回复。

## 用户消息

最终用户传入给模型消息，此时 `role` 字段应设置为 `user` ，该类型消息往往是包含用户希望模型处理的具体任务或者处理的信息。
下面就是一个简单的用户消息，要求模型对文本进行结构化处理。

```shell
messages = [
    {"role": "user", "content": "请将下面内容进行结构化处理：火山方舟是火山引擎推出的大模型服务平台，提供模型训练、推理、评测、精调等全方位功能与服务，并重点支撑大模型生态。 火山方舟通过稳定可靠的安全互信方案，保障模型提供方的模型安全与模型使用者的信息安全，加速大模型能力渗透到千行百业，助力模型提供方和使用者实现商业新增长。"}
]
Shell
```

## 系统消息

用于指定模型扮演角色或交代背景信息，此时 `role` 字段应设置为 `system` 。如设置系统消息，请放在 `messages` 列表的第一位。
下面是一个系统消息示例，模型会作为文本转化工具进行结构化处理。

```shell
messages =[
    {"role": "system", "content": "你是一个文本转化器，能够将输入的文本进行结构化处理。你收到信息后，只返回结构化处理后的内容，不应该返回其他内容。"},
    {"role": "user", "content": "请将下面内容进行结构化处理：火山方舟是火山引擎推出的大模型服务平台，提供模型训练、推理、评测、精调等全方位功能与服务，并重点支撑大模型生态。 火山方舟通过稳定可靠的安全互信方案，保障模型提供方的模型安全与模型使用者的信息安全，加速大模型能力渗透到千行百业，助力模型提供方和使用者实现商业新增长。"},
]
Shell
```

## 模型消息

假定为模型返回的消息，此时 `role` 字段应设置为 `assistant` 。在多轮对话中，会需要传入历史的对话，而模型回复的消息就可用模型消息表示。

```shell
messages =[
    {"role": "system", "content": "你是个十进制计算器，只返回计算结果，不返回其他"},
    {"role": "user", "content": "一加一"},
    {"role": "assistant", "content": "2"},
    {"role": "user", "content": "再加一"},
]
Shell
```

使用说明

- 每个模型输出有几个关键的限制，各个模型详细的规格信息，请参见 [模型列表](https://www.volcengine.com/docs/82379/1330310) 。
	- 最大上下文长度（Context Window）：即单次请求模型能处理的内容长度，包括用户输入和模型输出，单位 token 。超出最大上下文长度的内容时，会截断并停止输出。如碰到上下文限制导致的内容截断，可选择支持更大上下文长度规格的模型。
	- 最大输出长度（Max Tokens）：即单次模型输出的内容的最大长度。如碰到这种情况，可参考 [续写模式 Prefill Response](https://www.volcengine.com/docs/82379/1359497) ，通过多次续写回复，拼接出完整内容。
	- 每分钟处理内容量（TPM）：即账号下同模型（不区分版本）每分钟能处理的内容量限制，单位 token。如默认 TPM 限制无法满足您的业务，可通过 [工单](https://console.volcengine.com/workorder/create?step=2&SubProductID=P00001166) 联系售后提升配额。
		> 举例：某模型的 TPM 为 500w，一个主账号下创建的该模型的所有版本接入点共享此配额。
	- 每分钟处理请求数（RPM）：即账号下同模型（不区分版本）每秒钟能处理的请求数上限，与上面 TPM 类似。如默认 RPM 限制无法满足您的业务，可通过 [工单](https://console.volcengine.com/workorder/create?step=2&SubProductID=P00001166) 联系售后提升配额。
- 用量查询：
	- 对于某次请求 token 用量：可在返回的 **usage** 结构体中查看。
	- 输入/输出内容的 token 用量：可使用 [Tokenization API](https://www.volcengine.com/docs/82379/1528728) 或 [Token 计算器](https://console.volcengine.com/ark/region:ark+cn-beijing/tokenCalculator) 来估算。
	- 账号/项目/接入点维度 token 用量：可在 [用量统计](https://console.volcengine.com/ark/region:ark+cn-beijing/usageTracking) 页面查看。

代码示例

## 单轮对话

与模型进行一次交互，交互内容为单轮对话，模型根据系统消息和用户消息来返回内容。

> 因为是非流式输出，需要等待模型推理完所有内容，将内容一起返回给您，会有一定延时。

## 多轮对话

组合使用系统消息、模型消息以及用户消息，可实现多轮对话，即根据一个主题进行多次对话。
需要注意， `chat.completions` 接口是无状态的，在每次请求时，将历史信息都放在 `messages` 中，并通过 `role` 字段设置，让模型了解之前不同角色的不同对话内容，以便进行主题相关的延续性对话。

## 流式输出

动态输出内容，无需等待模型推理完毕，可实时看到中间输出过程内容。有如下好处：

- 缓解用户等待体感（一边输出一边看内容），效果如下所示。
- 模型推理事件，包括搜索、回答等，如豆包应用，可展示模型推理进展。
- 即使出错也可返回部分信息，而不会如非流式，直接失败。
- 保持客户端和服务端连接状态，避免模型处理复杂任务长时间推理未返回信息，导致客户端等待超时（SDK 默认等待 10分钟）而任务失败。
播放 暂停 进入全屏 退出全屏 00:00 00:00
- 2x
- 1.5x
- 1.25x
- 1x
- 0.75x
- 0.5x

1x

重播

## 工具调用

支持工具调用模型请参见 [支持范围](https://www.volcengine.com/docs/82379/1262342#8c325d45) ，更全面以及详细的工具调用使用教程请参见 [工具调用](https://www.volcengine.com/docs/82379/1827538) 。

## 异常处理

增加异常处理，帮助定位问题。

## 续写模式 Prefill Response

通过预填（Prefill）部分 `Assistant` 角色的内容，来引导和控制模型的输出。输出的控制可应用在多个方面：强制按照 JSON 或 XML 等特定格式输出；跳过已生成的内容，绕过模型最大输出限制；控制大模型在角色扮演场景中保持同一角色。

> 详细的场景使用说明，请参见 [续写模式 Prefill Response](https://www.volcengine.com/docs/82379/1359497) 。

## 对话加密

为了保证推理会话数据的传输安全，在默认的网络层加密方案基础上，为在线推理的会话数据提供了端到端应用层加密方案，更多能力介绍和原理信息请参见 [推理会话数据应用层加密方案](https://www.volcengine.com/docs/82379/1389905) 。
您可通过增加一行代码免费使用本功能。

> - 仅支持Python SDK，通过 `pip install 'volcengine-python-sdk[ark]' -U` 获得 SDK 的最新版本。
> - 仅支持豆包模型。
> - 仅支持 `ChatCompletions` 中的单轮/多轮会话，支持流式/非流式，同步/异步接口。

示例代码如下。

```python
import os
# Install SDK:  pip install 'volcengine-python-sdk[ark]'
from volcenginesdkarkruntime import Ark

# 初始化Ark客户端
client = Ark(
    # The base URL for model invocation
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    # Get API Key：https://console.volcengine.com/ark/region:ark+cn-beijing/apikey
    api_key=os.getenv('ARK_API_KEY'),
)

completion = client.chat.completions.create(
    # Replace with Model ID
    model = "doubao-seed-1-6-251015",
    messages = [
        {"role": "system", "content": "你是 AI 人工智能助手"},
        {"role": "user", "content": "常见的十字花科植物有哪些？"},
    ],
    #按下述代码设置自定义header，免费开启推理会话应用层加密
    extra_headers={'x-is-encrypted': 'true'}
)
print(completion.choices[0].message.content)
Python
```

## 设置最大回答长度

当您需要调整模型回答长度，如需控制成本，回答不超过 500 字；或回答篇幅较长，如翻译长文本，避免中途截断，可通过在请求时设置 `max_tokens` 字段，来达成目标。

> 控制模型输出长度（思维链＋回答），请参见 [设置模型输出长度限制](https://www.volcengine.com/docs/82379/1449737#31ecc4d7) 。

## 异步输出

当任务较为复杂或者多个任务并发等场景下，可使用 Asyncio 接口实现并发调用，提高程序的效率，优化体验。示例代码如下：

## 批量推理

方舟为您提供批量推理的能力，当您有大批量数据处理任务，可使用批量推理能力，以获得更大吞吐量和更低的成本。详细介绍和使用，请参见 [批量推理](https://www.volcengine.com/docs/82379/1399517) 。
